train:
  batch: 32
  epoch: 800
  iter: -1 #if -1, train until max
  lr: 0.0008
  save_interval: 200 #every epoch
  datatype: gesture
  timewindow: 3000 #Î¼s (=3ms =0.003s)
  sequence: 300 #limit the sequence
  timescales: [1.0] #only learn 1x
  schedule-step: -1 #if -1, no schedule
  schedule-rate: 0.5

  optim: AdamW #use AdamW with weight decay
  weight-decay: 0.01 #L2 regularization

model:
  type: dynamic-snn
  cnn-type: res #res for residual block CNN
  res-actfn: dyna-snn

  in-size: 32
  in-channel: 2 
  out-size: 11
  hiddens: [12,32] 
  pool-type: avg # avg to reduce information per step when speed is reduced
  pool-size: [2,2]
  residual-block: [1,1] #2 for 1+2 layers of CNN per residual block
  is-bn: false #whether to use batch normalization
  clip-norm: 1.0 #maximum gradient for gradient clipping. usually 1.0

  linear-hidden: 512

  dropout: 0.5

  dt: &dt 0.006 #s
  init-tau: 0.008 #25% memory
  min-tau: *dt
  v-threshold: 0.1 #threshold to make it big
  v-rest: 0.0
  reset-mechanism: zero
  spike-grad: fast-sigmoid
  output-membrane: false
  r: 1 #membrane resistance. this scales the input (input becomes smaller when dt is smaller)
  
  memory-lifstate: false