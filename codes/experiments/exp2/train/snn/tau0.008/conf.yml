train:
  batch: 32
  epoch: 800
  iter: -1 #ここは-1にするとmaxまで. とりあえず20くらいでいい
  lr: 0.0008
  save_interval: 200 #epochごと
  datatype: gesture
  timewindow: 3000 #μs (=3ms =0.003s)
  sequence: 300 #シーケンスは制限する
  timescales: [1.0] #dynaは1倍のみ学習
  schedule-step: -1 #-1でscheduleなし
  schedule-rate: 0.5

  optim: AdamW #weight decayを正しく実装したAdamWを使う
  weight-decay: 0.01 #L2正則化

model:
  type: snn
  cnn-type: res #resで残差ブロックCNNになる
  res-actfn: lif

  in-size: 32
  in-channel: 2 
  out-size: 11
  hiddens: [12,32] 
  pool-type: avg # avgにすることで,速度落としたときの1stepの情報量が落ちるようになる
  pool-size: [2,2]
  residual-block: [1,1] #2にするとresidual blockあたり1+2層のCNNが積まれる
  is-bn: false #batch normalizationするかどうか. 
  clip-norm: 1.0 #勾配クリッピングの最大勾配. 一般的に1.0が使われる

  linear-hidden: 512

  dropout: 0.5

  dt: &dt 0.006 #s
  init-tau: 0.008
  min-tau: *dt
  v-threshold: 0.1 #閾値を1にしてデカくする
  v-rest: 0.0
  reset-mechanism: zero
  spike-grad: fast-sigmoid
  output-membrane: false
  r: 1 #膜抵抗. これで入力がスケールされる(dtが小さいと入力が小さくなるのでその補正)
  train-tau: false
  is-bias: true
  
  memory-lifstate: false